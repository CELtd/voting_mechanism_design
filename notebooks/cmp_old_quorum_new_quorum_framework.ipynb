{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dcf0b5b",
   "metadata": {},
   "source": [
    "# Comparison of Old Quorum and New Quorum Frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85bd4820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'voting_mechanism_design'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvoting_mechanism_design\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mop_simulator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Simulation\n\u001b[0;32m     12\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mUserWarning\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'voting_mechanism_design'"
     ]
    }
   ],
   "source": [
    "\n",
    "        # Imports and Initialization\n",
    "        %load_ext autoreload\n",
    "        %autoreload 2\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import warnings\n",
    "        from tqdm.auto import tqdm\n",
    "        import itertools\n",
    "        import matplotlib as mpl\n",
    "        from voting_mechanism_design.legacy.op_simulator import Simulation\n",
    "        warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c49a814",
   "metadata": {},
   "source": [
    "## Simulation Functions for Old Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba86ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # Extracted and adapted from sim_fil.ipynb and simulator.py\n",
    "        def plot_distribution(project_df):    \n",
    "            fig, ax = plt.subplots(figsize=(8,5))\n",
    "            project_df['token_amount'].plot(kind='bar', width=1, ax=ax)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_xlabel(\"\")\n",
    "            ax.set_ylabel(\"Tokens\")\n",
    "            \n",
    "        def plot_alignment(project_df):    \n",
    "            fig, ax = plt.subplots(figsize=(8,5))\n",
    "            project_df.plot(kind='scatter', x='rating', y='token_amount', ax=ax)\n",
    "            ax.set_ylabel(\"Tokens\")\n",
    "            ax.set_xlabel(\"Impact\")\n",
    "            ax.set_xticks([])\n",
    "            \n",
    "        def plot_all(project_df, save_fp=None):\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(10,4), sharey=True)\n",
    "            project_df['token_amount'].plot(kind='bar', width=1, ax=ax[0])\n",
    "            project_df.plot(kind='scatter', x='rating', y='token_amount', ax=ax[1])\n",
    "            ax[0].set_xticks([])\n",
    "            ax[0].set_xlabel(\"\")\n",
    "            ax[0].set_ylabel(\"Pct Tokens Allocated\")\n",
    "            ax[1].set_ylabel(\"\")\n",
    "            ax[1].set_xlabel(\"Impact\")\n",
    "            ax[1].set_xticks([])\n",
    "            plt.tight_layout()\n",
    "            if save_fp:\n",
    "                plt.savefig(save_fp)\n",
    "            \n",
    "        def analyze_simulation(results, generate_plots=False):\n",
    "            summary = pd.Series(results).iloc[:-1].apply(lambda x: int(x) if isinstance(x, float) else x)\n",
    "            data = results['data']\n",
    "            project_df = pd.DataFrame(data).sort_values(by='token_amount', ascending=False)\n",
    "            if generate_plots:\n",
    "                plot_all(project_df)\n",
    "            return summary\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908ef640",
   "metadata": {},
   "source": [
    "## Simulation Functions for New Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af7038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # Extracted and adapted from quorum_example.ipynb and related scripts\n",
    "        # Similar to the functions for the old framework\n",
    "        # Assume we have similar functions for the new framework\n",
    "\n",
    "        def run_n_simulations(\n",
    "            n_sims,\n",
    "            n_badgeholders=50,\n",
    "            badgeholder_expertise_vec=None,\n",
    "            badgeholder_laziness_vec=None,\n",
    "            n_projects=100,\n",
    "            project_impact_vec=None,\n",
    "            quorum=5,\n",
    "            scoring_fn='mean',\n",
    "            min_vote_amt=1,\n",
    "            max_vote_amt=16,\n",
    "            max_funding=100,\n",
    "            random_seed_start=1234,\n",
    "            verbose=False,\n",
    "        ):\n",
    "            def run_simulation(ii):\n",
    "                seed = random_seed_start + ii\n",
    "                single_run_results = run_single_simulation(\n",
    "                    n_badgeholders=n_badgeholders,\n",
    "                    badgeholder_expertise_vec=badgeholder_expertise_vec,\n",
    "                    badgeholder_laziness_vec=badgeholder_laziness_vec,\n",
    "                    n_projects=n_projects,\n",
    "                    project_impact_vec=project_impact_vec,\n",
    "                    quorum=quorum,\n",
    "                    scoring_fn=scoring_fn,\n",
    "                    min_vote_amt=min_vote_amt,\n",
    "                    max_vote_amt=max_vote_amt,\n",
    "                    max_funding=max_funding,\n",
    "                    random_seed=seed\n",
    "                )\n",
    "                return {\n",
    "                    'Pairwise': single_run_results['metrics']['pairwise'],\n",
    "                    'Q+T': single_run_results['metrics']['qt'],\n",
    "                    'n_badgeholders': n_badgeholders,\n",
    "                    'avg_expertise': np.mean(badgeholder_expertise_vec),\n",
    "                    'avg_laziness': np.mean(badgeholder_laziness_vec),\n",
    "                    'n_projects': n_projects,\n",
    "                    'quorum': quorum,\n",
    "                    'scoring_fn': scoring_fn,\n",
    "                    'min_vote_amt': min_vote_amt,\n",
    "                    'max_vote_amt': max_vote_amt,\n",
    "                    'max_funding': max_funding,\n",
    "                }\n",
    "            \n",
    "            results = [run_simulation(ii) for ii in tqdm(range(n_sims), disable=(not verbose))]\n",
    "            return pd.DataFrame(results)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50256514",
   "metadata": {},
   "source": [
    "## Comparison Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca7eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # Define parameters for simulation\n",
    "        n_sims = 100\n",
    "        n_badgeholders = 50\n",
    "        n_projects = 100\n",
    "        badgeholder_expertise_vec = np.ones(n_badgeholders) * 0.8\n",
    "        badgeholder_laziness_vec = np.ones(n_badgeholders) * 0.2\n",
    "        quorum = 5\n",
    "        scoring_fn = 'mean'\n",
    "        min_vote_amt = 1\n",
    "        max_vote_amt = 16\n",
    "        max_funding = 100\n",
    "        random_seed_start = 1234\n",
    "        \n",
    "        # Run simulations for old framework\n",
    "        old_results = run_n_simulations(\n",
    "            n_sims=n_sims,\n",
    "            n_badgeholders=n_badgeholders,\n",
    "            badgeholder_expertise_vec=badgeholder_expertise_vec,\n",
    "            badgeholder_laziness_vec=badgeholder_laziness_vec,\n",
    "            n_projects=n_projects,\n",
    "            project_impact_vec=np.linspace(0, 1, n_projects),\n",
    "            quorum=quorum,\n",
    "            scoring_fn=scoring_fn,\n",
    "            min_vote_amt=min_vote_amt,\n",
    "            max_vote_amt=max_vote_amt,\n",
    "            max_funding=max_funding,\n",
    "            random_seed_start=random_seed_start,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Run simulations for new framework\n",
    "        new_results = run_n_simulations(\n",
    "            n_sims=n_sims,\n",
    "            n_badgeholders=n_badgeholders,\n",
    "            badgeholder_expertise_vec=badgeholder_expertise_vec,\n",
    "            badgeholder_laziness_vec=badgeholder_laziness_vec,\n",
    "            n_projects=n_projects,\n",
    "            project_impact_vec=np.linspace(0, 1, n_projects),\n",
    "            quorum=quorum,\n",
    "            scoring_fn=scoring_fn,\n",
    "            min_vote_amt=min_vote_amt,\n",
    "            max_vote_amt=max_vote_amt,\n",
    "            max_funding=max_funding,\n",
    "            random_seed_start=random_seed_start,\n",
    "            verbose=True\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc288d2",
   "metadata": {},
   "source": [
    "## Analysis and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f7e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # Analysis and plotting\n",
    "        def plot_comparison(old_results, new_results):\n",
    "            df_old = pd.melt(\n",
    "                old_results, \n",
    "                value_vars=['Pairwise', 'Q+T'], value_name='Rank Corr', var_name='Mechanism',\n",
    "                id_vars=['n_badgeholders', 'avg_expertise', 'avg_laziness', 'n_projects', 'quorum']\n",
    "            )\n",
    "            df_old['Framework'] = 'Old'\n",
    "            \n",
    "            df_new = pd.melt(\n",
    "                new_results, \n",
    "                value_vars=['Pairwise', 'Q+T'], value_name='Rank Corr', var_name='Mechanism',\n",
    "                id_vars=['n_badgeholders', 'avg_expertise', 'avg_laziness', 'n_projects', 'quorum']\n",
    "            )\n",
    "            df_new['Framework'] = 'New'\n",
    "            \n",
    "            combined_df = pd.concat([df_old, df_new])\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.boxplot(data=combined_df, x='Mechanism', y='Rank Corr', hue='Framework')\n",
    "            plt.title('Comparison of Old and New Quorum Frameworks')\n",
    "            plt.show()\n",
    "        \n",
    "        plot_comparison(old_results, new_results)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
