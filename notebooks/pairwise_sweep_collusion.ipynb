{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from voting_mechanism_design.agents.pairwise_badgeholder import PairwiseBadgeholder, PairwiseBadgeholderPopulation\n",
    "from voting_mechanism_design.projects.project import Project, ProjectPopulation\n",
    "from voting_mechanism_design.funds_distribution.pairwise_binary import PairwiseBinary\n",
    "from voting_mechanism_design.sim import RoundSimulation\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import choix\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a metric used for comparing the inferred ranking to the actual ranking\n",
    "def min_swaps_to_sort(arr1, arr2):\n",
    "    n = len(arr1)\n",
    "    # Create a map of value to its index in arr1\n",
    "    index_map = {value: i for i, value in enumerate(arr1)}\n",
    "\n",
    "    visited = [False] * n\n",
    "    swaps = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        if visited[i] or arr1[i] == arr2[i]:\n",
    "            continue\n",
    "\n",
    "        cycle_size = 0\n",
    "        x = i\n",
    "\n",
    "        while not visited[x]:\n",
    "            visited[x] = True\n",
    "            x = index_map[arr2[x]]\n",
    "            cycle_size += 1\n",
    "\n",
    "        if cycle_size > 0:\n",
    "            swaps += (cycle_size - 1)\n",
    "\n",
    "    return swaps\n",
    "\n",
    "def run_single_simulation(\n",
    "        n_badgeholders=50, \n",
    "        badgeholder_expertise_distribution=None,  # a vector input that should be len==#badgeholders\n",
    "        badgeholder_voting_style='skewed_towards_impact',\n",
    "        badgeholder_laziness_distribution=None,   # a vector input that should be len==#badgeholders\n",
    "        n_projects=20, \n",
    "        project_impact_distribution=None,         # a vector input that should be len==#projects\n",
    "        badgeholder_coi_mappings=None,  # a dictionary of badgeholder_id to project_id that is COI\n",
    "        coi_factor=1.0,\n",
    "        random_seed=1234\n",
    "    ):\n",
    "    assert badgeholder_expertise_distribution is not None\n",
    "    assert badgeholder_laziness_distribution is not None\n",
    "    assert project_impact_distribution is not None\n",
    "    assert len(badgeholder_expertise_distribution) == n_badgeholders, f'{len(badgeholder_expertise_distribution)} != {n_badgeholders}'\n",
    "    assert len(badgeholder_laziness_distribution) == n_badgeholders, f'{len(badgeholder_laziness_distribution)} != {n_badgeholders}'\n",
    "    assert len(project_impact_distribution) == n_projects, f'{len(project_impact_distribution)} != {n_projects}'\n",
    "\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "\n",
    "    if badgeholder_coi_mappings is None:\n",
    "        badgeholder_coi_mappings = {}\n",
    "    # this loop error checking assumes that badgeholderIds and projectIds are 0-indexed\n",
    "    for badgeholder_id, project_ids in badgeholder_coi_mappings.items():\n",
    "        if badgeholder_id >= n_badgeholders:\n",
    "            raise ValueError('COI badgeholder_id must be less than n_badgeholders')\n",
    "        for project_id in project_ids:\n",
    "            if project_id >= n_projects:\n",
    "                raise ValueError('COI project_id must be less than n_projects')\n",
    "    \n",
    "    # get a list of all the projects that will be voted via COI\n",
    "    coi_projects_list = []\n",
    "    for badgeholder_id, project_ids in badgeholder_coi_mappings.items():\n",
    "        coi_projects_list.extend(project_ids)\n",
    "\n",
    "    # create badgeholders\n",
    "    badgeholder_list = []\n",
    "    for ii in range(n_badgeholders):\n",
    "        badgeholder_id = ii\n",
    "        expertise = badgeholder_expertise_distribution[ii]\n",
    "        laziness = badgeholder_laziness_distribution[ii]\n",
    "        \n",
    "        if badgeholder_id in badgeholder_coi_mappings:\n",
    "            coi_project_ids = badgeholder_coi_mappings[badgeholder_id]\n",
    "            coi_factor_i = coi_factor\n",
    "            # print(f'Badgeholder {badgeholder_id} is engaging in COI with projects: {coi_project_ids}')\n",
    "        else:\n",
    "            coi_project_ids = []\n",
    "            coi_factor_i = 0\n",
    "        agent = PairwiseBadgeholder(\n",
    "            badgeholder_id=badgeholder_id,\n",
    "            voting_style=badgeholder_voting_style,\n",
    "            voting_style_kwargs={'use_impact_delta':False},\n",
    "            expertise=expertise,\n",
    "            laziness=laziness,\n",
    "            coi_project_ix_vec=coi_project_ids,\n",
    "            coi_factor=coi_factor_i,\n",
    "        )\n",
    "        badgeholder_list.append(agent)\n",
    "    badgeholders = PairwiseBadgeholderPopulation()\n",
    "    badgeholders.add_badgeholders(badgeholder_list)\n",
    "\n",
    "    # create projects\n",
    "    projects = []\n",
    "    for ii in range(n_projects):\n",
    "        project_id = ii\n",
    "        project_impact = project_impact_distribution[ii]\n",
    "        project = Project(\n",
    "            project_id=project_id,\n",
    "            true_impact=project_impact,  \n",
    "            owner_id=None,  # used for COI modeling\n",
    "        )\n",
    "        projects.append(project)\n",
    "    project_population = ProjectPopulation()\n",
    "    project_population.add_projects(projects)\n",
    "\n",
    "    # this is a no-op currently, but can change in the future\n",
    "    fund_distribution_model = PairwiseBinary()\n",
    "\n",
    "    # create and run a simulation\n",
    "    simulation_obj = RoundSimulation(\n",
    "        badgeholder_population=badgeholders,\n",
    "        projects=project_population,\n",
    "        funding_design=fund_distribution_model,\n",
    "        random_seed=random_seed\n",
    "    )\n",
    "\n",
    "    # voters can vote on all projects, but an individual badgeholder can decide to ignore\n",
    "    # some, based on the badgeholder configuration\n",
    "    voting_view_ix = list(itertools.combinations(range(project_population.num_projects), 2))  # we need a list since we use this twice\n",
    "    voting_view = []\n",
    "    for ix1, ix2 in voting_view_ix:\n",
    "        voting_view.append((project_population.get_project(ix1), project_population.get_project(ix2)))\n",
    "    simulation_obj.run(cast_votes_kwargs={'view': voting_view})\n",
    "\n",
    "    # store the project impact differences\n",
    "    impact_diff_vec = []\n",
    "    for ix1, ix2 in voting_view_ix:\n",
    "        p1 = project_population.get_project(ix1)\n",
    "        p2 = project_population.get_project(ix2)\n",
    "        impact_diff_vec.append(p1.true_impact - p2.true_impact)\n",
    "\n",
    "    # make a matrix of all the pairwise voting possibilities\n",
    "    all_votes = simulation_obj.badgeholder_population.get_all_votes()\n",
    "    project_list = simulation_obj.projects.projects\n",
    "    N = len(project_list)\n",
    "    project_vote_matrix = np.zeros((N, N))\n",
    "    project2ix = {p:ix for ix, p in enumerate(project_list)}\n",
    "    vote_data_list = []\n",
    "\n",
    "    # put the votes into the matrix for analysis\n",
    "    for v in all_votes:\n",
    "        ix1 = project2ix[v.project1]\n",
    "        ix2 = project2ix[v.project2]\n",
    "        project_vote_matrix[ix1, ix2] += v.val1\n",
    "        project_vote_matrix[ix2, ix1] += v.val2\n",
    "\n",
    "        if v.val1 > v.val2:\n",
    "            vote_data_list.append((ix1, ix2))\n",
    "        elif v.val2 > v.val1:\n",
    "            vote_data_list.append((ix2, ix1))\n",
    "        else:\n",
    "            # TODO: not sure if there is a tie, is that even possible??\n",
    "            print('In Tie scenario!')\n",
    "            pass\n",
    "\n",
    "    # estimate parameters of bradley terry model - TODO: update to MCMC estimation\n",
    "    try:\n",
    "        params = choix.ilsr_pairwise(N, vote_data_list)  \n",
    "    except ValueError:\n",
    "        params = choix.ilsr_pairwise(N, vote_data_list, alpha=0.01)\n",
    "    # create rank order of project ids\n",
    "    project_ix_worst_to_best_inferred = np.argsort(params)\n",
    "    project_ix_best_to_worst_inferred = project_ix_worst_to_best_inferred[::-1]\n",
    "\n",
    "    # compare the inferred ranks to the actual ranks\n",
    "    actual_project_impacts = np.array([p.true_impact for p in project_list])\n",
    "    # rank them\n",
    "    project_ix_worst_to_best_actual = np.argsort(actual_project_impacts)\n",
    "    project_ix_best_to_worst_actual = project_ix_worst_to_best_actual[::-1]\n",
    "\n",
    "    # # compare the two\n",
    "    # print('Actual Project Ranking by impact:', project_ix_worst_to_best_actual)\n",
    "    # print('Inferred Project Ranking by impact:', project_ix_worst_to_best_inferred)\n",
    "\n",
    "    min_swaps = min_swaps_to_sort(project_ix_worst_to_best_actual, project_ix_worst_to_best_inferred)\n",
    "    tau = stats.kendalltau(project_ix_worst_to_best_actual, project_ix_worst_to_best_inferred).correlation\n",
    "\n",
    "    return min_swaps, tau, impact_diff_vec, project_ix_worst_to_best_actual, project_ix_worst_to_best_inferred\n",
    "\n",
    "def run_n_simulations(\n",
    "    n_sims,\n",
    "    n_badgeholders, \n",
    "    badgeholder_expertise_distribution,\n",
    "    badgeholder_voting_style,\n",
    "    badgeholder_laziness_distribution,\n",
    "    n_projects, \n",
    "    project_impact_distribution,\n",
    "    badgeholder_coi_mappings,\n",
    "    coi_factor=1.0,\n",
    "    random_seed_start=1234,\n",
    "    n_jobs=-1  # Number of parallel jobs (-1 means using all processors)\n",
    "):\n",
    "    def run_simulation(ii):\n",
    "        seed = random_seed_start + ii\n",
    "        min_swaps, ktau, impact_diff_vec, project_ix_worst_to_best_actual, project_ix_worst_to_best_inferred = run_single_simulation(\n",
    "            n_badgeholders=n_badgeholders, \n",
    "            badgeholder_expertise_distribution=badgeholder_expertise_distribution,\n",
    "            badgeholder_voting_style=badgeholder_voting_style,\n",
    "            badgeholder_laziness_distribution=badgeholder_laziness_distribution,\n",
    "            n_projects=n_projects, \n",
    "            project_impact_distribution=project_impact_distribution,\n",
    "            badgeholder_coi_mappings=badgeholder_coi_mappings,\n",
    "            coi_factor=coi_factor,\n",
    "            random_seed=seed\n",
    "        )\n",
    "        return min_swaps, ktau, impact_diff_vec, project_ix_worst_to_best_actual, project_ix_worst_to_best_inferred\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(run_simulation)(ii) for ii in tqdm(range(n_sims), disable=True))\n",
    "    # results = []\n",
    "    # for ii in tqdm(range(n_sims), disable=True):\n",
    "    #     results.append(run_simulation(ii))\n",
    "    \n",
    "    min_swaps_list, ktau_list, impact_diff_vec, project_ix_worst_to_best_actual, project_ix_worst_to_best_inferred = zip(*results)\n",
    "    min_swaps_list = np.array(min_swaps_list)\n",
    "    ktau_list = np.array(ktau_list)\n",
    "    impact_diff_vec = np.array(impact_diff_vec)\n",
    "    project_ix_worst_to_best_actual = np.array(project_ix_worst_to_best_actual)\n",
    "    project_ix_worst_to_best_inferred = np.array(project_ix_worst_to_best_inferred)\n",
    "\n",
    "    return {\n",
    "        'min_swaps_list': min_swaps_list,\n",
    "        'ktau_list': ktau_list,\n",
    "        'impact_diff_vec': impact_diff_vec,\n",
    "        'project_ix_worst_to_best_actual': project_ix_worst_to_best_actual,\n",
    "        'project_ix_worst_to_best_inferred': project_ix_worst_to_best_inferred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mc_coiindex(\n",
    "    num_mc = 32,\n",
    "    n_badgeholders = 30,\n",
    "    n_projects = 100,\n",
    "    badgeholder_voting_style = 'skewed_towards_impact',\n",
    "    coi_project_ix = 25,\n",
    "    coi_badgeholder_ix_vec = [0],\n",
    "    coi_factor=1.0,\n",
    "\n",
    "    # set these directly so we can more directly measure the impact of COI\n",
    "    project_impact_distribution=None,\n",
    "    badgeholder_expertise_distribution=None,\n",
    "    badgeholder_laziness_distribution=None,\n",
    "\n",
    "    seed_start=1234,\n",
    "):\n",
    "    if project_impact_distribution is None:\n",
    "        raise ValueError(\"project_impact_distribution must be set\")\n",
    "    if badgeholder_expertise_distribution is None:\n",
    "        raise ValueError(\"badgeholder_expertise_distribution must be set\")\n",
    "    if badgeholder_laziness_distribution is None:\n",
    "        raise ValueError(\"badgeholder_laziness_distribution must be set\")\n",
    "    \n",
    "    badgeholder_coi_mapping = {}\n",
    "    for coi_badgeholder_ix in coi_badgeholder_ix_vec:\n",
    "        badgeholder_coi_mapping[coi_badgeholder_ix] = [coi_project_ix]\n",
    "\n",
    "    sim_results_with_coi = run_n_simulations(\n",
    "        num_mc,\n",
    "        n_badgeholders=n_badgeholders, \n",
    "        badgeholder_expertise_distribution=badgeholder_expertise_distribution,\n",
    "        badgeholder_voting_style=badgeholder_voting_style,\n",
    "        badgeholder_laziness_distribution=badgeholder_laziness_distribution,\n",
    "        n_projects=n_projects, \n",
    "        project_impact_distribution=project_impact_distribution,\n",
    "        badgeholder_coi_mappings=badgeholder_coi_mapping,\n",
    "        coi_factor=coi_factor,\n",
    "        random_seed_start=seed_start,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    sim_results_without_coi = run_n_simulations(\n",
    "        num_mc,\n",
    "        n_badgeholders=n_badgeholders, \n",
    "        badgeholder_expertise_distribution=badgeholder_expertise_distribution,\n",
    "        badgeholder_voting_style=badgeholder_voting_style,\n",
    "        badgeholder_laziness_distribution=badgeholder_laziness_distribution,\n",
    "        n_projects=n_projects, \n",
    "        project_impact_distribution=project_impact_distribution,\n",
    "        badgeholder_coi_mappings=None,\n",
    "        coi_factor=0.0,\n",
    "        random_seed_start=seed_start,  # same seed to make results directly comparable\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    actual_minus_inferred_with_coi_allprojects = sim_results_with_coi['project_ix_worst_to_best_actual'] - sim_results_with_coi['project_ix_worst_to_best_inferred']\n",
    "    actual_minus_inferred_without_coi_allprojects = sim_results_without_coi['project_ix_worst_to_best_actual'] - sim_results_without_coi['project_ix_worst_to_best_inferred']\n",
    "\n",
    "    # coi_project_ixx = np.where(sim_results_with_coi['project_ix_worst_to_best_actual'] == coi_project_ix)\n",
    "    # print(coi_project_ix)\n",
    "    # print(sim_results_with_coi['project_ix_worst_to_best_actual'])\n",
    "    # print(coi_project_ixx)\n",
    "\n",
    "    coi_project_inferred_ix_with_coi = np.where(sim_results_with_coi['project_ix_worst_to_best_inferred'] == coi_project_ix)\n",
    "    coi_project_inferred_ix_without_coi = np.where(sim_results_without_coi['project_ix_worst_to_best_inferred'] == coi_project_ix)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'sim_results_with_coi': sim_results_with_coi,\n",
    "        'sim_results_without_coi': sim_results_without_coi,\n",
    "        'actual_minus_inferred_with_coi_allprojects': actual_minus_inferred_with_coi_allprojects,\n",
    "        'actual_minus_inferred_without_coi_allprojects': actual_minus_inferred_without_coi_allprojects,\n",
    "        'coi_project_ix': coi_project_ix,\n",
    "        'coi_project_inferred_ix_with_coi': coi_project_inferred_ix_with_coi,\n",
    "        'coi_project_inferred_ix_without_coi': coi_project_inferred_ix_without_coi\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c8c6581cb94979baaad77ab81fde4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x111704790>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/rpgf/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "num_mc = 32\n",
    "n_badgeholders = 30\n",
    "n_projects = 100\n",
    "badgeholder_voting_style = 'skewed_towards_impact'\n",
    "\n",
    "project_impact_distribution = np.linspace(0.1, 0.9, n_projects)\n",
    "badgeholder_expertise_vec = [0.25, 0.5]\n",
    "badgeholder_laziness_vec = [0.5, 0.75]\n",
    "\n",
    "coi_index = [0, 50, 99]\n",
    "coi_factor_vec = [0.5, 0.75, 1.0]\n",
    "collusion_count_vec = [1, 5, 10]\n",
    "experiment_cfgs = list(itertools.product(\n",
    "    coi_index,\n",
    "    coi_factor_vec,\n",
    "    collusion_count_vec,\n",
    "    badgeholder_expertise_vec,\n",
    "    badgeholder_laziness_vec,\n",
    "))\n",
    "\n",
    "exp2results = {}\n",
    "for experiment_cfg in tqdm(experiment_cfgs):\n",
    "    # print(f'Running COI index: {coi_ix}')\n",
    "    coi_ix, coi_factor, collusion_count, expertise, laziness = experiment_cfg\n",
    "    badgeholder_expertise_distribution = np.ones(n_badgeholders) * expertise\n",
    "    badgeholder_laziness_distribution = np.ones(n_badgeholders) * laziness\n",
    "    coi_badgeholder_ix_vec = np.arange(collusion_count)+1  # the indices of the badgeholders who are colluding\n",
    "    res = run_mc_coiindex(\n",
    "        num_mc = num_mc,\n",
    "        n_badgeholders = n_badgeholders,\n",
    "        n_projects = n_projects,\n",
    "        badgeholder_voting_style = badgeholder_voting_style,\n",
    "        coi_project_ix = coi_ix,\n",
    "        coi_badgeholder_ix_vec=coi_badgeholder_ix_vec, \n",
    "        coi_factor=coi_factor,\n",
    "\n",
    "        # set these directly so we can more directly measure the impact of COI\n",
    "        project_impact_distribution=project_impact_distribution,\n",
    "        badgeholder_expertise_distribution=badgeholder_expertise_distribution,\n",
    "        badgeholder_laziness_distribution=badgeholder_laziness_distribution,\n",
    "\n",
    "        seed_start=1234,\n",
    "    )\n",
    "    exp2results[experiment_cfg] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make into a melted df\n",
    "results_list = []\n",
    "for experiment_cfg in experiment_cfgs: \n",
    "    coi_ix, coi_factor, collusion_count, expertise, laziness = experiment_cfg\n",
    "    coi_project_inferred_ix_without_coi = np.asarray(exp2results[experiment_cfg]['coi_project_inferred_ix_without_coi'][1])\n",
    "    coi_project_inferred_ix_with_coi = np.asarray(exp2results[experiment_cfg]['coi_project_inferred_ix_with_coi'][1])\n",
    "\n",
    "    delta = coi_project_inferred_ix_with_coi - coi_project_inferred_ix_without_coi\n",
    "    \n",
    "    for d in delta:\n",
    "        results_list.append({\n",
    "            'coi_ix': coi_ix,\n",
    "            'coi_factor': coi_factor,\n",
    "            'expertise': expertise,\n",
    "            'laziness': laziness,\n",
    "            'collusion_count': collusion_count,\n",
    "            'delta': d,\n",
    "            'hue': '%0.02f/%0.02f/%0.02f' % (expertise, laziness)\n",
    "        })\n",
    "df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the color pallete\n",
    "color_range = np.linspace(0.2, 0.8, len(badgeholder_laziness_vec))\n",
    "colors=['Blues', 'Greens', 'Reds', 'Purples']\n",
    "palette = {}\n",
    "coi_factor = 0.5 # TODO: put this into a sweep\n",
    "collusion_count = 5\n",
    "for jj, m in enumerate(badgeholder_expertise_vec):\n",
    "    color = mpl.colormaps[colors[jj]]\n",
    "    for ii, l in enumerate(badgeholder_laziness_vec):\n",
    "        key='%0.02f/%0.02f' % (m, l)\n",
    "        palette[key] = color(color_range[len(badgeholder_laziness_vec)-ii-1])\n",
    "\n",
    "\n",
    "ax = sns.stripplot(data=df, x='coi_ix', y='delta', hue='hue', \n",
    "                   dodge=True, palette=palette, zorder=1,)  # TODO: custom legend\n",
    "plt.ylabel('Project Ranking Delta (with COI - without COI)')\n",
    "ax.set_xticklabels(['Least Impactful Project', '', '', '', 'Most Impactful Project'])\n",
    "ax.set_xlabel('')\n",
    "ax.legend(title='Expertise/Laziness', bbox_to_anchor=(1.05, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make a plot of how the overall project ranking is updated by COI\n",
    "\n",
    "# # this is for the whole population\n",
    "# actual_minus_inferred_with_coi = sim_results_with_coi['project_ix_worst_to_best_actual'] - sim_results_with_coi['project_ix_worst_to_best_inferred']\n",
    "# actual_minus_inferred_without_coi = sim_results_without_coi['project_ix_worst_to_best_actual'] - sim_results_without_coi['project_ix_worst_to_best_inferred']\n",
    "\n",
    "# # actual_minus_inferred_with_coi\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(5, 3))\n",
    "# sns.histplot(actual_minus_inferred_with_coi.flatten(), ax=ax, label='With COI')\n",
    "# sns.histplot(actual_minus_inferred_without_coi.flatten(), ax=ax, label='Without COI')\n",
    "# ax.set_title('Actual - Inferred Project Ranking')\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rpgf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
