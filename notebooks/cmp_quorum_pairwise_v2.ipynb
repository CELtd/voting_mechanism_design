{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# from simulator import Simulation\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import matplotlib as mpl\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "from voting_mechanism_design.agents.pairwise_badgeholder import PairwiseBadgeholder, PairwiseBadgeholderPopulation\n",
    "from voting_mechanism_design.projects.project import Project, ProjectPopulation\n",
    "from voting_mechanism_design.funds_distribution.pairwise_binary import PairwiseBinary\n",
    "from voting_mechanism_design.sim import RoundSimulation\n",
    "\n",
    "from voting_mechanism_design.agents.quorum_badgeholder import QuorumBadgeholder, QuorumBadgeholderPopulation\n",
    "from voting_mechanism_design.funds_distribution.threshold_and_aggregate import ThresholdAndAggregate\n",
    "\n",
    "# # OP simulator configuration\n",
    "# from voting_mechanism_design.legacy.op_simulator import Simulation as OPSimulation\n",
    "# from voting_mechanism_design.legacy.op_simulator import Round as OPRound\n",
    "# from voting_mechanism_design.legacy.op_simulator import Voter as OPVoter\n",
    "# from voting_mechanism_design.legacy.op_simulator import Project as OPProject\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import choix\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_monotonic_array(max_val, min_val, length, total_sum):\n",
    "    x = np.linspace(max_val, min_val, length)\n",
    "    x /= x.sum()\n",
    "    x *= total_sum\n",
    "    \n",
    "    # if any values exceed the first_value, we need to adjust the values\n",
    "    mm1 = x[0]\n",
    "    mm2 = x[-1]\n",
    "    if mm1 > max_val:\n",
    "        delta = mm1 - max_val\n",
    "    else:\n",
    "        delta = 0\n",
    "    x = np.linspace(mm1-delta, mm2+delta, length)\n",
    "    x /= x.sum()\n",
    "    x *= total_sum\n",
    "\n",
    "    return x\n",
    "\n",
    "def run_single_quorum_threshold_simulation(\n",
    "        n_badgeholders=50,\n",
    "        badgeholder_expertise_vec=None,\n",
    "        badgeholder_laziness_vec=None,\n",
    "        n_projects=100,\n",
    "        project_impact_vec=None,\n",
    "        # configuration specific to quorum+threshold voting method\n",
    "        quorum=5,\n",
    "        scoring_fn='mean',\n",
    "        min_vote_amt=1,\n",
    "        max_vote_amt=16,\n",
    "        max_funding=100,\n",
    "        random_seed=1234\n",
    "    ):\n",
    "    # create voters\n",
    "    voters = []\n",
    "    for ii in range(n_badgeholders):\n",
    "        assert badgeholder_laziness_vec[ii] >= 0\n",
    "        v = QuorumBadgeholder(\n",
    "            badgeholder_id=ii,\n",
    "            total_funds=max_funding,\n",
    "            min_vote=min_vote_amt,  \n",
    "            max_vote=max_vote_amt,\n",
    "            laziness=badgeholder_laziness_vec[ii],\n",
    "            expertise=badgeholder_expertise_vec[ii],\n",
    "            coi_factor=0,\n",
    "            expertise_model=\"k2\"\n",
    "        )\n",
    "        voters.append(v)\n",
    "    badgeholder_pop=QuorumBadgeholderPopulation()\n",
    "    badgeholder_pop.add_badgeholders(voters)\n",
    "\n",
    "    # add projects\n",
    "    projects = []\n",
    "    for ii in range(n_projects):\n",
    "        p = Project(\n",
    "            project_id=ii,\n",
    "            true_impact=project_impact_vec[ii],  \n",
    "            owner_id=None,  \n",
    "        )\n",
    "        projects.append(p)\n",
    "    project_population = ProjectPopulation()\n",
    "    project_population.add_projects(projects)\n",
    "\n",
    "    # ##  Voting Simulation\n",
    "    fund_distribution_model = ThresholdAndAggregate()\n",
    "    simulation_obj = RoundSimulation(\n",
    "        badgeholder_population=badgeholder_pop,\n",
    "        projects=project_population,\n",
    "        funding_design=fund_distribution_model,\n",
    "    )\n",
    "    projects = project_population.get_projects()\n",
    "    simulation_obj.run(cast_votes_kwargs={'projects': projects})\n",
    "\n",
    "    # extract the scores\n",
    "    project_population = simulation_obj.projects\n",
    "    project_vote_counts = defaultdict(int)\n",
    "    for project in projects:\n",
    "        for vote in project.votes:\n",
    "            # project_vote_counts[project.project_id] += round(vote.amount if vote.amount is not None else 0, 10)\n",
    "            project_vote_counts[project.project_id] += vote.amount if vote.amount is not None else 0\n",
    "    project_funds_df = pd.DataFrame({\n",
    "        'Project Id': list(project_vote_counts.keys()),\n",
    "        'Total Funds': list(project_vote_counts.values())\n",
    "    })\n",
    "    sorted_df = project_funds_df.sort_values(by='Total Funds', ascending=False).reset_index(drop=True)\n",
    "    project_rankings = sorted_df['Project Id'].tolist()\n",
    "    return _, sorted_df\n",
    "    # TODO: we need a dictionary\n",
    "\n",
    "    # return project_funds_df\n",
    "\n",
    "    # projectid2score = round.calculate_scores(\n",
    "    #     scoring_fn, \n",
    "    #     quorum\n",
    "    # )\n",
    "    # projectid2score = OrderedDict(projectid2score)\n",
    "    # ranks = np.argsort(np.asarray(list(projectid2score.values())))\n",
    "    # projectid2rank = {ii:ranks[ii] for ii in range(len(ranks))}\n",
    "    \n",
    "    # return projectid2score, projectid2rank\n",
    "\n",
    "def run_single_pairwise_simulation(\n",
    "        n_badgeholders=50,\n",
    "        badgeholder_expertise_vec=None,\n",
    "        badgeholder_laziness_vec=None,\n",
    "        n_projects=100,\n",
    "        project_impact_vec=None,\n",
    "        random_seed=1234\n",
    "    ):\n",
    "    # create badgeholders\n",
    "    badgeholder_list = []\n",
    "    for ii in range(n_badgeholders):\n",
    "        badgeholder_id = ii\n",
    "        expertise = badgeholder_expertise_vec[ii]\n",
    "        laziness = badgeholder_laziness_vec[ii]\n",
    "        \n",
    "        coi_project_ids = []\n",
    "        coi_factor = 0.0\n",
    "        agent = PairwiseBadgeholder(\n",
    "            badgeholder_id=badgeholder_id,\n",
    "            voting_style='skewed_towards_impact',\n",
    "            voting_style_kwargs={'use_impact_delta':False},\n",
    "            expertise=expertise,\n",
    "            laziness=laziness,\n",
    "            coi_project_ix_vec=coi_project_ids,\n",
    "            coi_factor=coi_factor,\n",
    "        )\n",
    "        badgeholder_list.append(agent)\n",
    "    badgeholders = PairwiseBadgeholderPopulation()\n",
    "    badgeholders.add_badgeholders(badgeholder_list)\n",
    "\n",
    "    # create projects\n",
    "    projects = []\n",
    "    for ii in range(n_projects):\n",
    "        project_id = ii\n",
    "        project_impact = project_impact_vec[ii]\n",
    "        project = Project(\n",
    "            project_id=project_id,\n",
    "            true_impact=project_impact,  \n",
    "            owner_id=None,  # used for COI modeling\n",
    "        )\n",
    "        projects.append(project)\n",
    "    project_population = ProjectPopulation()\n",
    "    project_population.add_projects(projects)\n",
    "\n",
    "    # this is a no-op currently, but can change in the future\n",
    "    fund_distribution_model = PairwiseBinary()\n",
    "\n",
    "    # create and run a simulation\n",
    "    simulation_obj = RoundSimulation(\n",
    "        badgeholder_population=badgeholders,\n",
    "        projects=project_population,\n",
    "        funding_design=fund_distribution_model,\n",
    "        random_seed=random_seed\n",
    "    )\n",
    "\n",
    "    # voters can vote on all projects, but an individual badgeholder can decide to ignore\n",
    "    # some, based on the badgeholder configuration\n",
    "    voting_view_ix = list(itertools.combinations(range(project_population.num_projects), 2))  # we need a list since we use this twice\n",
    "    voting_view = []\n",
    "    for ix1, ix2 in voting_view_ix:\n",
    "        voting_view.append((project_population.get_project(ix1), project_population.get_project(ix2)))\n",
    "    simulation_obj.run(cast_votes_kwargs={'view': voting_view})\n",
    "\n",
    "    # store the project impact differences\n",
    "    # this is only for diagnostics/post-hoc analysis\n",
    "    impact_diff_vec = []\n",
    "    for ix1, ix2 in voting_view_ix:\n",
    "        p1 = project_population.get_project(ix1)\n",
    "        p2 = project_population.get_project(ix2)\n",
    "        impact_diff_vec.append(p1.true_impact - p2.true_impact)\n",
    "\n",
    "    # make a matrix of all the pairwise voting possibilities\n",
    "    all_votes = simulation_obj.badgeholder_population.get_all_votes()\n",
    "    project_list = simulation_obj.projects.projects\n",
    "    N = len(project_list)\n",
    "    project_vote_matrix = np.zeros((N, N))\n",
    "    project2ix = {p:ix for ix, p in enumerate(project_list)}\n",
    "    vote_data_list = []\n",
    "\n",
    "    # put the votes into the format needed by the choix package\n",
    "    for v in all_votes:\n",
    "        ix1 = project2ix[v.project1]\n",
    "        ix2 = project2ix[v.project2]\n",
    "        project_vote_matrix[ix1, ix2] += v.val1\n",
    "        project_vote_matrix[ix2, ix1] += v.val2\n",
    "\n",
    "        if v.val1 > v.val2:\n",
    "            vote_data_list.append((ix1, ix2))\n",
    "        elif v.val2 > v.val1:\n",
    "            vote_data_list.append((ix2, ix1))\n",
    "        else:\n",
    "            # TODO: not sure if there is a tie, is that even possible??\n",
    "            print('In Tie scenario!')\n",
    "            pass\n",
    "\n",
    "    # estimate parameters of bradley terry model - TODO: update to MCMC estimation\n",
    "    try:\n",
    "        params = choix.ilsr_pairwise(N, vote_data_list)  \n",
    "    except Exception as e:\n",
    "        # print(f'Failed to estimate global rankings: {e}, adding some regularization!')\n",
    "        params = choix.ilsr_pairwise(N, vote_data_list, alpha=0.01)\n",
    "    # create rank order of project ids\n",
    "    project_ix_worst_to_best_inferred = np.argsort(params)\n",
    "    project_ix_best_to_worst_inferred = project_ix_worst_to_best_inferred[::-1]\n",
    "\n",
    "    # # compare the inferred ranks to the actual ranks\n",
    "    # actual_project_impacts = np.array([p.true_impact for p in project_list])\n",
    "    # # rank them\n",
    "    # project_ix_worst_to_best_actual = np.argsort(actual_project_impacts)\n",
    "    # project_ix_best_to_worst_actual = project_ix_worst_to_best_actual[::-1]\n",
    "\n",
    "    # return actual_project_impacts, project_ix_worst_to_best_inferred\n",
    "    return params, project_ix_worst_to_best_inferred, impact_diff_vec\n",
    "\n",
    "def run_single_simulation(\n",
    "        n_badgeholders=50,\n",
    "        badgeholder_expertise_vec=None,\n",
    "        badgeholder_laziness_vec=None,\n",
    "        n_projects=100,\n",
    "        project_impact_vec=None,\n",
    "        quorum=5,\n",
    "        scoring_fn='mean',\n",
    "        min_vote_amt=1,\n",
    "        max_vote_amt=16,\n",
    "        max_funding=100,\n",
    "        random_seed=1234\n",
    "    ):\n",
    "    qt_project_scores, qt_project_ranks = run_single_quorum_threshold_simulation(\n",
    "        n_badgeholders=n_badgeholders,\n",
    "        badgeholder_expertise_vec=badgeholder_expertise_vec,\n",
    "        badgeholder_laziness_vec=badgeholder_laziness_vec,\n",
    "        n_projects=n_projects,\n",
    "        project_impact_vec=project_impact_vec,\n",
    "        # configuration specific to quorum+threshold voting method\n",
    "        quorum=quorum,\n",
    "        scoring_fn=scoring_fn,\n",
    "        min_vote_amt=min_vote_amt,\n",
    "        max_vote_amt=max_vote_amt,\n",
    "        max_funding=max_funding,\n",
    "        random_seed=random_seed\n",
    "    )\n",
    "    # raise Exception(\"Test\")\n",
    "\n",
    "    # NOTE: this is a simple version which excludes\n",
    "    # COI information.  We can build up the simulation to add this\n",
    "    \n",
    "    # NOTE: this is a way to align the laziness between Q+T and Pairwise\n",
    "    # it is imperfect at best, and we need to do some more thinking to determine\n",
    "    # how better to align this, but this is a starting point.  The thinking here\n",
    "    # is that a badgeholder of laziness = 0.1 in Q+T is roughly equivalent to a\n",
    "    # a badgeholder of laziness = 0.01 in pairwise, a 10x conversion factor. The conversion\n",
    "    badgeholder_laziness_vec_pw = np.power(badgeholder_laziness_vec, 0.2)\n",
    "\n",
    "    pairwise_project_scores, pairwise_project_ranks, impact_diff_vec = run_single_pairwise_simulation(\n",
    "        n_badgeholders=n_badgeholders,\n",
    "        badgeholder_expertise_vec=badgeholder_expertise_vec,\n",
    "        badgeholder_laziness_vec=badgeholder_laziness_vec_pw,\n",
    "        n_projects=n_projects,\n",
    "        project_impact_vec=project_impact_vec,\n",
    "        random_seed=random_seed\n",
    "    )\n",
    "\n",
    "    # the results to return, but others may contain useful diagnostics as well!\n",
    "    pairwise_inferred_rankings = pairwise_project_ranks\n",
    "    # qt_inferred_rankings = np.asarray(list(qt_project_ranks.values()))\n",
    "    qt_inferred_rankings = []\n",
    "    for ii in range(n_projects):\n",
    "        qt_inferred_rankings.append(qt_project_ranks[ii])\n",
    "    qt_inferred_rankings = np.asarray(qt_inferred_rankings)\n",
    "    true_project_rankings = np.argsort(project_impact_vec)\n",
    "    \n",
    "    # compute rank correlation as an initial measure of how aligned \n",
    "    tau_pairwise = stats.kendalltau(true_project_rankings, pairwise_inferred_rankings)\n",
    "    tau_qt = stats.kendalltau(true_project_rankings, qt_inferred_rankings)\n",
    "\n",
    "    return {\n",
    "        'rankings': {\n",
    "            'pairwise': pairwise_inferred_rankings,\n",
    "            'qt': qt_inferred_rankings,\n",
    "            'true': true_project_rankings\n",
    "        },\n",
    "        'metrics': {\n",
    "            # index-0 is the actual statistic, index-1 is a p-value or something\n",
    "            'pairwise': tau_pairwise[0],\n",
    "            'qt': tau_qt[0]\n",
    "        }\n",
    "    }\n",
    "\n",
    "def run_n_simulations(\n",
    "        n_sims,\n",
    "        n_badgeholders=50,\n",
    "        badgeholder_expertise_vec=None,\n",
    "        badgeholder_laziness_vec=None,\n",
    "        n_projects=100,\n",
    "        project_impact_vec=None,\n",
    "        quorum=5,\n",
    "        scoring_fn='mean',\n",
    "        min_vote_amt=1,\n",
    "        max_vote_amt=16,\n",
    "        max_funding=100,\n",
    "        random_seed_start=1234,\n",
    "        verbose=False,\n",
    "    ):\n",
    "    def run_simulation(ii):\n",
    "        seed = random_seed_start + ii\n",
    "        single_run_results = run_single_simulation(\n",
    "            n_badgeholders=n_badgeholders,\n",
    "            badgeholder_expertise_vec=badgeholder_expertise_vec,\n",
    "            badgeholder_laziness_vec=badgeholder_laziness_vec,\n",
    "            n_projects=n_projects,\n",
    "            project_impact_vec=project_impact_vec,\n",
    "            quorum=quorum,\n",
    "            scoring_fn=scoring_fn,\n",
    "            min_vote_amt=min_vote_amt,\n",
    "            max_vote_amt=max_vote_amt,\n",
    "            max_funding=max_funding,\n",
    "            random_seed=seed\n",
    "        )\n",
    "        # return a dict so that we can create a dataframe of the results\n",
    "        # for each configuration we test quickly\n",
    "        return_dict = {\n",
    "            'Pairwise': single_run_results['metrics']['pairwise'],\n",
    "            'Q+T': single_run_results['metrics']['qt'],\n",
    "            # 'pw_rank': single_run_results['rankings']['pairwise'],\n",
    "            # 'qt_rank': single_run_results['rankings']['qt'],\n",
    "            'n_badgeholders': n_badgeholders,\n",
    "            'avg_expertise': np.mean(badgeholder_expertise_vec),\n",
    "            'avg_laziness': np.mean(badgeholder_laziness_vec),\n",
    "            'n_projects': n_projects,\n",
    "            'quorum': quorum,\n",
    "            'scoring_fn': scoring_fn,\n",
    "            'min_vote_amt': min_vote_amt,\n",
    "            'max_vote_amt': max_vote_amt,\n",
    "            'max_funding': max_funding,\n",
    "        }\n",
    "        return return_dict\n",
    "    \n",
    "    results = Parallel(n_jobs=-1)(delayed(run_simulation)(ii) for ii in tqdm(range(n_sims), disable=(not verbose)))\n",
    "    # results = []\n",
    "    # for ii in tqdm(range(n_sims), disable=(not verbose)):\n",
    "    #     results.append(run_simulation(ii))\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test w/ some random values to start\n",
    "n_badgeholders = 25\n",
    "n_projects = 100\n",
    "badgeholder_expertise_vec = 0.25*np.ones(n_badgeholders)\n",
    "badgeholder_laziness_vec = 0.25*np.ones(n_badgeholders)\n",
    "project_impact_vec=np.linspace(0, 1, n_projects)  # enforce a strict ranking that we can compare against\n",
    "\n",
    "quorum = 5\n",
    "scoring_fn = 'mean'\n",
    "min_vote_amt = 1\n",
    "max_vote_amt = 16\n",
    "max_funding = 100\n",
    "random_seed = 1234\n",
    "\n",
    "qt_project_scores, qt_project_ranks = run_single_quorum_threshold_simulation(\n",
    "        n_badgeholders=n_badgeholders,\n",
    "        badgeholder_expertise_vec=badgeholder_expertise_vec,\n",
    "        badgeholder_laziness_vec=badgeholder_laziness_vec,\n",
    "        n_projects=n_projects,\n",
    "        project_impact_vec=project_impact_vec,\n",
    "        # configuration specific to quorum+threshold voting method\n",
    "        quorum=quorum,\n",
    "        scoring_fn=scoring_fn,\n",
    "        min_vote_amt=min_vote_amt,\n",
    "        max_vote_amt=max_vote_amt,\n",
    "        max_funding=max_funding,\n",
    "        random_seed=random_seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project Id</th>\n",
       "      <th>Total Funds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>37.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>37.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>37.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>37.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>37.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>83</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>76</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>71</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>89</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Project Id  Total Funds\n",
       "0            1         37.5\n",
       "1           17         37.5\n",
       "2           10         37.5\n",
       "3            4         37.5\n",
       "4           34         37.5\n",
       "..         ...          ...\n",
       "95          75          0.0\n",
       "96          83          0.0\n",
       "97          76          0.0\n",
       "98          71          0.0\n",
       "99          89          0.0\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qt_project_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup simulation parameters\n",
    "n_sims = 32\n",
    "\n",
    "n_badgeholders_sweep = [25]\n",
    "n_projects_sweep = [100]\n",
    "badgeholder_expertise_sweep = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "badgeholder_laziness_sweep = [0.0, 0.25, 0.5, 0.75]\n",
    "\n",
    "experiment_cfgs = list(itertools.product(\n",
    "    n_badgeholders_sweep,\n",
    "    n_projects_sweep,\n",
    "    badgeholder_expertise_sweep,\n",
    "    badgeholder_laziness_sweep\n",
    "))\n",
    "\n",
    "# parameters that we can explore\n",
    "quorum=5\n",
    "scoring_fn='mean'\n",
    "min_vote_amt=1\n",
    "max_vote_amt=16\n",
    "max_funding=100\n",
    "\n",
    "all_results = []\n",
    "for experiment_cfg in tqdm(experiment_cfgs):\n",
    "    n_badgeholders=experiment_cfg[0]\n",
    "    n_projects=experiment_cfg[1]\n",
    "\n",
    "    badgeholder_expertise_vec=np.ones(n_badgeholders) * experiment_cfg[2]\n",
    "    badgeholder_laziness_vec=np.ones(n_badgeholders) * experiment_cfg[3]\n",
    "\n",
    "    random_seed_start=1234\n",
    "    project_impact_vec=np.linspace(0, 1, n_projects)  # enforce a strict ranking that we can compare against\n",
    "\n",
    "    results_df = run_n_simulations(\n",
    "        n_sims,\n",
    "        n_badgeholders=n_badgeholders,\n",
    "        badgeholder_expertise_vec=badgeholder_expertise_vec,\n",
    "        badgeholder_laziness_vec=badgeholder_laziness_vec,\n",
    "        n_projects=n_projects,\n",
    "        project_impact_vec=project_impact_vec,\n",
    "        quorum=quorum,\n",
    "        scoring_fn=scoring_fn,\n",
    "        min_vote_amt=min_vote_amt,\n",
    "        max_vote_amt=max_vote_amt,\n",
    "        max_funding=max_funding,\n",
    "        random_seed_start=random_seed_start,\n",
    "        verbose=False\n",
    "    )\n",
    "    all_results.append(results_df)\n",
    "all_results = pd.concat(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.melt(\n",
    "    all_results, \n",
    "    value_vars=['Pairwise', 'Q+T'], value_name='Rank Corr', var_name='Mechanism',\n",
    "    id_vars=['n_badgeholders', 'avg_expertise', 'avg_laziness', 'n_projects', 'quorum']\n",
    ")\n",
    "dff['Hue'] = ''\n",
    "for ii, row in dff.iterrows():\n",
    "    out_str = '%s//%0.02f' % (row['Mechanism'], row['avg_laziness'],)\n",
    "    dff.at[ii, 'Hue'] = out_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create palette\n",
    "mechanisms = ['Pairwise', 'Q+T']\n",
    "laziness_vec = [0.0, 0.25, 0.5, 0.75]\n",
    "color_range = np.linspace(0.8, 0.2, len(laziness_vec))\n",
    "colors=['Blues', 'Greens', 'Reds', 'Purples']\n",
    "palette = {}\n",
    "for jj, m in enumerate(mechanisms):\n",
    "    color = mpl.colormaps[colors[jj]]\n",
    "    for ii, l in enumerate(laziness_vec):\n",
    "        key='%s//%0.02f' % (m, l,)\n",
    "        palette[key] = color(color_range[ii])\n",
    "\n",
    "ax = sns.stripplot(\n",
    "    data=dff, x=\"avg_expertise\", y=\"Rank Corr\", hue=\"Hue\",\n",
    "    dodge=True, zorder=1, legend=False, palette=palette,\n",
    ")\n",
    "plt.xlabel('Expertise')\n",
    "plt.ylabel('Alignment')\n",
    "# plt.legend(title='Mechanism//Laziness')\n",
    "# create custom legend\n",
    "for ii, m in enumerate(mechanisms):\n",
    "    ax.scatter([], [], color=mpl.colormaps[colors[ii]](0.8), label=mechanisms[ii])\n",
    "for ii, l in enumerate(laziness_vec):\n",
    "    ax.scatter([], [], color=mpl.colormaps['Greys'](color_range[ii]), label='Lz=%0.02f' % l)\n",
    "ax.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on Alignment for Q+T\n",
    "1. Q+T has less alignment than Pairwise.  Even if the expertise is normalized, it is less b/c\n",
    "   the user must also assign funding amounts to each project.  This introduces an additional source\n",
    "   of randomness.\n",
    "2. Even if it is not random (i.e. top ranked project gets top amount and then work down), all badgeholder's views are aggregated and this introduces some noise in the alignment.\n",
    "3. Interestingly, as badgeholder laziness goes up in the Q+T scheme, the alignment seems to get better. Likely b/c they vote on less projects, so there is less ambiguity in assigning projects closer amounts of funding, causing\n",
    "4. It really depends on how a Q+T badgeholder assigns funding to the projects after they have ranked them.\n",
    "5. This means the above plot may not be completely fair in terms of comparison, in the sense that pairwise is comparing alignment directly, where as Q+T is including the scoring aspect.  That being said, that is sort of the mechanism, since in pairwise, we only want rankings and then we create an a-priori mapping to funding amount, so this is sort of an advantage in the sense that it creates a clearer signal.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
